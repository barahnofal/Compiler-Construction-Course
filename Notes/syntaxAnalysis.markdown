# Syntax Analysis(Parser)

A Syntax analyzer is formally defined as :

> An Algorithm that Groups the Set of Tokens Sent by the Scanner to Form
> **Syntax Structures** Such As Expressions, Statements, Blocks,etc.

Simply put, the parser examines if the source code written follows
the grammar(production rules) of the language.


The Syntax structure of programming languages and even spoken languages
can be expressed in what is called **BNF** notation, which stands 
for **B**akus **N**aur **F**orm. 

For example, in spoken English, we can say the following:

> sentence --> noun-phrase	verb-phrase
>
> noun-phrase --> article	noun 
>
> article --> THE | A | ...
> 
> noun --> STUDENT | BOOK | ...
>
> verb-phrase --> verb noun-phrase 
>
> verb --> READS | BUYS | ....

Note : The BNF Notation uses [different symbols](https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form#Example),
for example, a sentence is defined as :

> \< sentence \> ::= \< noun-phrase \>	\< verb-phrase \>

But this is very cumbersome, so we use the first notation, since its
easier to use. 

Now, let us derive a sentence : 

> sentence --> **noun-phrase** verb-phrase 
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> **article** noun verb-phrase
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE **noun** verb-phrase
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT **verb-phrase**
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT **verb** noun-phrase
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS **noun-phrase**
> 
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS **article** noun
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS A **noun**
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS A BOOK


In the same way, the parser tries to **derive** your source program 
from the starting symbol of the grammar.

Lets say we have these sentences :

> THE BOOK BUYS A STUDENT
>
> THE BOOK WRITES A DISH
>
> THE DISH TAKES A STROLL

Syntax-wise, all of these sentences are correct. However, their meaning 
is not correct, and they are not useful. What differentiates 2
sentences that are grammatically correct is their meaning or their 
**semantics**. You and I can agree that the meaning of a grammatically 
correct sentence is not correct, but how does the computer do it?



## Grammar 

> A grammar G=(V<sub>N</sub>, V<sub>T</sub>, S, P) where:
>
> 1. V<sub>N</sub> : A finite set of nonterminals(nonterminals set).
> 2. V<sub>T</sub> : A finite set of terminals(terminals set).
> 3. S &isin; V<sub>N</sub> : The Starting symbol of the grammar. 
> 4. P =  A set of **production rules**(productions).<-- Pending <==> Basically the whole grammar.

Note :

1. V<sub>N</sub> &cap; V<sub>T</sub> = &empty;.
2. V<sub>N</sub> &cup; V<sub>T</sub> = V(the vocabulary of the grammar).

Note : We will use 

1. Uppercase Letters A,B,...,Z for non-terminals.

2. Lowercase Letters a,b,...,z for terminals.

3. Greek letters &alpha;,&beta;,&gamma;,... for strings formed from V<sub>N</sub> OR V<sub>T</sub> = V. eg, 

   if V<sub>N</sub> = {S,A,B},
   
   V<sub>T</sub> = {0,1}
   
   then
   
   &alpha; = 0A11B
   
   &beta; = S110B
   
   &gamma; = 0010

### Productions 

1. A Production &alpha; --> &beta;(alpha derives beta) is a rewriting rule such that
the occurrence of &alpha; can be substituted by &beta; in any string.

   Note that &alpha; must contain at least one nonterminal from,&isin;V<sub>N</sub>. 

   For example, Assume we have the string &gamma;&alpha;&sigma;,

   > &gamma;&alpha;&sigma; --> &gamma;&beta;&sigma;
   
2. A Derivation is a sequence of strings &alpha;<sub>0</sub>, &alpha;<sub>1</sub>,
&alpha;<sub>2</sub>, &alpha;<sub>3</sub>,....,&alpha;<sub>n</sub>, then :

	- &alpha;<sub>0</sub> -*-> &alpha;<sub>n</sub>, n &ge; 0.
	
	- &alpha;<sub>0</sub> -<sup>+</sup>-> &alpha;<sub>n</sub>, n &ge; 1.
	

Given a grammar G, then :

> L(G) = Language Generated By the Grammar.
	
for example, Given the Grammar, G = ({S,B,C},{a,b,c},S,P)

P :

> S --> aSBC 
>
> S --> abC
>
> CB --> BC
>
> bB --> bb
>
> bC --> bc
>
> cC --> CC

L(G)=?

Lets follow through on the derivations

> S --> a**bC** --> abc(all terminals) &isin; L(G) <--- A sentence
>
> S --> a**S**BC --> aa**bC**BC --> aabbcBC --> blocked, so we try another path
> 
> S --> a**S**BC --> aab**CB**C --> aa**bB**CC --> aab**bC**C --> aabb**cC** --> aabbcc &isin; L(G) <--- A sentence
>
> S --> a**S**BC -->........-->aaabbbccc &isin; L(G) <--- A sentence
>
> Therefore, L(G)={a<sup>n</sup>,b<sup>n</sup>,c<sup>n</sup>| n &ge; 1}


As another Example, we have these productions

> E --> E+T <-- we can write the productions 1 and 2 as a single production E --> E+T | T
>
> E --> T
>
> T --> T*F
>
> T --> F
>
> F --> (E) <-- we can write the productions 5 and 6 as a single production F --> (E) | n
>
> F --> n

Lets follow through some derivations

> E --> **T** --> **F** --> n &isin; L(E)
>
> E --> **E**+T --> T+**T** --> T+**F** --> **T**+n --> **F**+n --> n+n &isin; L(E)
>
> E --> **E**+T --> **T**+T --> **F**+T --> n+**T** --> n+**F** --> n + (**E**) --> n+(**T**)
> --> n+(**T**\*F) --> n+(**F**\*F) --> n+(n\***F**) --> n+(n\*n) &isin; L(E)
>
> Therefore, L(G) = {Any arithmetic expression with \* and + operations},
> n is an operand here.

Note that, if we add the productions

> E --> E+T | E-T | T
>
> T --> T\*F | T/F | T%F

We would have a language to express all arithmetic expressions with 
(\*,\\,+,\-) operations.

Lets Take another Example(things in double quotes are terminals)

> Program --> block "#"
>
> block --> "{" stmt-List "}"
>
> stmt-List --> statement ";" stmt-List | &lambda;
>
> statement --> if-stmt | while-stmt | read-stmt | write-stmt |
> assignment-stmt | block
>
> if-stmt --> "if" condition.... 
>
> while-stmt --> "while" condition.....
>
> ....
> 
> ....
> 
> read-stmt --> "read"
>
> write-stmt --> "write"


V<sub>N</sub> = {Program, block, stmt-List, statement, if-stmt,
while-stmt, read-stmt, write-stmt, assignment-stmt}

V<sub>T</sub> = { "{", "}", "#", ";", "if", "while", "read", "write" }


Lets Follow through some derivations :

> Program --> **block** # --> { **stmt-list** } # --> { &lambda; } #
>
> Program --> **block** # --> {**stmt-list**} # --> {statement ; **stmt-list**} #
> --> {statement ; statement ; **stmt-list**} # --> {statement ; statement ; &lambda;} #
> --> {**statement** ; statement ;} # -->  {**READ-statement** ; statement ;} #
> -->{READ ; **statement** ;} # -->{READ ; write-statement ;} # --> {READ ; WRITE ;} #

We can write this as 

```
{ READ;
  WRITE;
}#
```
The language of this language is defined as 

> L(G) = {Set of all programs that can be written in this language}.

This is only a simple example, of a simple language. For something more
complex such as C or Pascal, there are hundreds of productions.


### Algorithms for Derivation 


>A Leftmost derivation is a derivation in which we replace the **leftmost**
>nonterminal in each derivation step.


>A Rightmost derivation is a derivation in which we replace the **rightmost**
>nonterminal in each derivation step.

For example, given the grammar

>V --> S R $
>
>S --> +|-|&lambda;
>
>R --> .dN | dN.N
>
>N --> dN | &lambda;
>
>V<sub>N</sub> = {V,R,S,N}
>
>V<sub>T</sub> = {+, - , ., d, $}

Lets follow through on the leftmost derivation 

> V --> **S**R$ --> -**R**$ --> -d**N**.N$ --> -dd**N**.N$ --> -dddN.N$
> --> -ddd.**N**$ --> -ddd.d**N**$ --> -ddd.d$ <-- A sentence.

Lets follow through on the rightmost derivation

> V --> S**R**$ --> SdN.**N** --> SdN.d**N**$ --> Sd**N**.d$ --> sdd**N**.d$
> --> sddd**N**.d$ --> **S**ddd.d$ --> -ddd.d$ <-- A sentence.

### Derivation Trees

A Derivation Tree is a Tree that displays the derivation of some 
sentence in the language. For example, lets look at the 
tree for the previous example

**INSERT IMAGE OF TREE FOR -ddd.d$**

Note that if we traverse the tree in order, recording **only** the leaves,
we obtain the sentence.

### Classes of Grammars 

According to Chomsky, There are 4 classes of grammars :

1. Unrestricted Grammars : No restrictions whatsoever except the restriction
by definition that the left side of the production contains at least one
nonterminal from V<sub>s</sub>. This grammar is not practical and we cannot
work with it. 

2. Context-Sensitive Grammars : For each production &alpha; --> &Beta;,
|&alpha;| &le; |&Beta;|, ie , the **length of alpha(&alpha;)** is less than or equal to 
the **length of Beta(&Beta;)**. This means that in this class of grammar, there are no
&lambda; productions in the form A --> &lambda, since |&lambda;| = 0 and A &ge; 1.

   They Say that Fortran has a context-sensitive grammar.
   
   It is very difficult to work with this class of grammars.

3. Context-Free Grammar(CFG) : Each production in this grammar class is of the 
form A --> &alpha; , where A &isin; V<sub>N</sub> and &alpha; &isin; V<sub><sub>*</sub></sub>
that is to say, the left hand side is **only** one nonterminal.

   This is the most important class of grammar. Most programming languages's
   structures are context-free. 
   
   We will mostly be working with this class of grammar. Most of the
   examples we have taken are CFG.

4. Regular Grammar (Regular Expressions) : Each production in this grammar class
   is of the form A -- > aB or A --> a, where A,B &isin; V<sub>N</sub> and a &isin;
   V<sub>T</sub>, **with the exception** of S --> &lambda;
   
   For example, lets say we have the grammar
   
   > A --> aA
   >
   > A --> a
   >
   Therefore, we get 
   
   >
   >G(L)=a<sup>+</sup>
   > 
	
   However, adding the production 
   
   >
   > A --> &lambda;
   >
   
   Results in the grammar
   
   >
   >G(L)=a\*
   > 

## Parsing Techniques

There are 2 main parsing techniques used by a compiler.

### Top-Down Parsing

In Top-Down Parsing, the parser builds the derivation tree from the 
root(S : the starting symbol) down to the leaves(sentence). 

In Simple words, the parser tries to derive the sentence using 
leftmost derivation. For example, say we have this grammar :

> V --> SR$
>
> S --> + | - | &lambda;
>
> R --> .dN | dN.N
>
> N --> dN | &lambda;

Lets examine if the sentence

> dd.d$ 

is derived from this grammar.

> V --> **S**R$ --> +**R**$ --> d**N**.N$ --> dd**N**.N$ --> dd.**N**$ --> dd.d**N**$ --> dd.d$

Therefore, this sentence is derived from the grammar.

However, this approach is very computationally intensive, and more importantly, 
this requires knowing the source code in advance. The Parser doesnt know
which production it should select in each derivation statement. We will
learn how to solve these issues later in the course.

### Bottom-Up Parsing

In Bottom-Up Parsing, the parser builds the derivation tree from the
leaves(sentence) up to the root(S : Starting Symbol). This type of tree,
built from the leaves to the root, is 
called a [B-Tree](https://en.wikipedia.org/wiki/B-tree).

In Simple words, the parser starts with the given sentence, does 
**reduction**(opposite of derivation) steps, until the starting symbol
is reached.

Note that the string &lambda; is present everywhere in the string, and
we can use it wherever we like.

Lets follow the reduction of the example given above.

> +dd.d$ --> +dd&lambda;.d$ --> +ddN.d$ --> +dN.d$ --> +dN.d&lambda;$
> --> +dN.dN$ --> +dN.N$ --> +R$ --> SR$ --> V

Which means that the sentence is in the grammar. 

Note that we can run into deadlocks here. say we took this path instead :

> +dd.d$ --> +dd&lambda;.d$ --> +ddN.d$ --> +dN.d$ --> +dN.d&lambda;$
> --> +dN.dN$ --> **+dNR$ --> +NR$ --> SNR$** --> Deadlock

This technique also has a major problem : Which substring should we 
select to reduce in each reduction step?

how do we slove this?

## Ambiguity 

Given the following grammar :

> num --> num d
>
> num --> d

Let us draw the derivation tree for the sentence ```dddd```

** TODO INSERT TREE**

Question : is there another derivation tree that represents the sentence?

The answer is **no**.

If there is only one derivation tree representing the sentence, 
this means there is only one way to derive the sentence.

Based on this, we can say that :

> A Grammar G is said to be ambiguous if there is one sentence with more than
> one derivation tree. 
> 
> That is, there is more than one way to derive the sentence. 
>
> This means that our algorithm is **non-deterministic**.

Say we have this grammar

> E --> E + E
>
> E --> E * E
>
> E --> (E) | a

Take the sentence :

> a + a * a

Lets draw the derivation tree

**TODO INSERT DERIVATION TREE 1 and 2**


Due to the fact that we have 2 trees that give the same result, we can 
say that this grammar is ambiguous.

In this case, to enforce the associativity rule, this grammar can 
be re-written as :

> E --> E + E | T 
>
> T --> T*T | F
>
> F--> (E) | a

Now, Take the sentence ```a + a * a```
and find the derivation tree now. 

** INSERT NEW DERIVATION TREE **

There is only 1 possible derivation tree now. This solves the associativity
issue of the grammar before with the ```+``` and ```*``` operations.

But lets say we have the sentence :

> a + a + a

Lets try to find the derivation tree and any alternative trees. 

**TODO INSERT DERIVATION TREES**

We can see here that there is more than 1 derivation tree, and the 
language is still ambiguous.

We can solve this if we rewrite the grammar with the **left-associative rule**

> E --> E + T | T
>
> T --> T * F | F
>
> F --> (E) | a

The resultant grammar is left-associative.

This grammar solves the problems of :

- ambiguity.
- precedence.
- associativity.

Lets try rewriting it with the **right-associative rule**

> E --> T + E | T
>
> T --> F * T | F
>
> F --> (E) | a

Lets try creating the derivation tree of ```a + a * a```

** INSERT THE TREE of a+a*a**

Now lets draw the derivation tree of ```a + a + a```

** INSERT THE TREE OF a+a+a**

This new grammar is not ambiguious, however, as we can tell from the derivation 
trees, there are precedence issues now. It's not technically wrong, 
but it doesnt not follow standard arithmetic rules.

Back to the left-associative grammar now. This grammar is called
**left-recursive**. This causes problems when it omes to top-down parsing
techniques(we will see why later).

A grammar is said to be left recursive if there is a production of the form:

> A-->A&alpha;

Conversely, a grammar is right-recursive if there is a production of the form:

> A-->&alpha;A

And causes no problems in top-down parsing.

our grammar has 2 rules of the form 

>A-->A&alpha;

The solution is to transform the grammar to a grammar which is not 
left-recursive. 

This has an algorithm to it. 

Given that 

> A-->A&alpha;<sub>1</sub>| A&alpha;<sub>2</sub>|A&alpha;<sub>3</sub>|...|A&alpha;<sub>n</sub>
>
> A-->&Beta;<sub>1</sub>|&Beta;<sub>2</sub>|&Beta;<sub>3</sub>|...|&Beta;<sub>n</sub>

To do this, we must introduce a new non-terminal, say A\`.

The grammar now becomes :

> A-->&Beta;<sub>1</sub>A\`|&Beta;<sub>2</sub>A\`|&Beta;<sub>3</sub>A\`|...|&Beta;<sub>n</sub>A\`

and

> A\`-->&alpha;<sub>1</sub>A\`| &alpha;<sub>2</sub>A\`|&alpha;<sub>3</sub>A\`|...|&alpha;<sub>n</sub>A\`|&lambda;

For example, say we have

>A-->Ab
>
>A-->a
>
>L(G)=ab*

Then according to the above

>A-->aA\`
>
>A\`-->bA\`|&lambda;

which results in the same grammar.

Lets apply this to the grammar :

> E --> E + T | T
>
> T --> T * F | F
>
> F --> (E) | a

This results in :

> E --> T E\` 
>
> E\` --> + T E\` | &lambda;
>
> T --> F T\`
>
> T\` --> \* F T\` | &lambda;
>
> F --> (E) | a

This grammar is now **perfect**. It solves all our ambiguity issues, and this is a
grammar we can use to construct the production rules for our programming
language.

Another ambiguity in programming languages is the ```if...else```
statement.

Lets take a generic if statement in a generic language:

> stmt --> if-stmt | while-stmt | ....
>
> if-stmt --> IF condition stmt
>
> if-stmt --> IF condition stmt ELSE stmt 
>
> condition --> C
>
> stmt --> S 

This grammar is ambiguous.

Lets take the nested ```if...else``` statement :

```
IF C
	IF C
		S
	ELSE
		S
		
```
This statement results in 2 derivations trees :

**INSERT TREES OF THIS STATEMENT**

Both these trees result in the same traversal, but they have different meanings.
The first results in the ```ELSE``` belonging to the first ```IF```, while the 
second results int he ```ELSE``` belonging to the second ```IF```. We as 
humans know that the ```ELSE``` belongs to the second ```IF```, since we know that
the ```ELSE``` statement follows the nearest ```IF```. but how can 
the compiler know?

There are a bunch of solutions to this problem:

1. Add a delimiter to the ```IF``` statement, such as ```ENDIF``` or 
```END``` or ```FI``` to the end of the statement, resulting in these 
productions :
> if-stmt --> IF condition stmt **ENDIF**
>
> if-stmt --> IF condition stmt ELSE stmt **ENDIF**

	Resulting in this statement :
	
	```
	IF C
	|	IF C
	|	|	S
	|	|ELSE
	|	|	S
	|	ENDIF
	ENDIF
	
	```
	The grammar is now unambigious, since we have to clearly state when 
	an ```IF``` statement ends. However, this is not a pretty solution, 
	and is extra work for both the programmar and compiler, and results
	in less readable code.
2. In C and Pascal, the compiler **always** prefers to shift the ```ELSE```
	when it sees it in the source code so it follows the nearest ```IF```. We 
	will learn about this in more detail later.
	
Another thing about this grammar is **left factoring**. 

### Left Factoring
Consider the productions :

> A --> &alpha;&beta;
>
> A --> &alpha;&gamma;

Note how the first part of the productions is the same. This grammar
can be transformed by introducing a new non-terminal, So what happens now is:

> A --> &alpha;B
>  
> B --> &beta;&gamma;

For our grammar, this results in 

> if-stmt --> IF condition stmt
>
> if-stmt --> IF condition stmt ELSE stmt 

becoming

> if-stmt --> IF conditon stmt else-part
>
> else-part --> ELSE stmt | &lambda;

Does this solve the ambiguity? No, but it helps in removing choices, since 
the if-stmt is now one production. If we look at the statement :


```
IF C
	IF C
		S
	ELSE
		S
		
```

It still has 2 derivation trees

**TODO INSERT THE 2 DERIVATION TREES**

## More Ways of Expressing Programming Languages

### Extended BNF Notation

So far, we have been using **BNF Notation**(Production rules) to express
languages. However, there is another form to Express a language, which is 
**Extended BNF Notation**

if there is repetition in the grammar, say in the example of the grammar


> E --> E + T | T
> 
> T --> T \* F | F
>
> F --> (E) | a

which can give us a derivation in the form of 

> E --> **E** + T --> **E** + T + T --> **E** + T + T + T --> T + T + T + T....+T

or in the same line, 

> T --> T * F --> T * F * F --> T * F * F * F --> T * F * F * F....* F

We can express this grammar as :

> E --> T { + T }
>
> T --> F { * F }
>
> F --> (E) | a

We know that \[x\] means that we take x 0 or 1 time only.

However, \{ x \} means we take x zero or any number of times. This is 
equivalent to \(x\)\*

We can also express this grammar as:

> E --> T (+ T)\*
>
> T --> F (* F)\*
>
> F --> (E) | a

### Syntax Diagrams

Another way to express languages are **Syntax Diagrams**. These are used
only with Extended-BNF notation.

Lets say we have want to express the Expression **E**, using the Terms **T**
and the Factors **F** of the form 

**TODO INSERT DRAWINGS OF SYNTAX TREES**.

As we can see, A square shape represents a nonterminal and an oval shape
represents a terminal.

## Parsing Techniques (Continued)

Recall : The parser is an algorithm which accepts or rejects a sentence
in the programming language. 

Recall : There are 2 kinds of parsers : 

1. Top-Down Parsers : In This parsing technique, The parser starts with 
```S``` using leftmost derivation to derive the sentence. The Major problem
with this parsing technique is that the parser doesn't know which production
it should select in each derivation step.

2. Bottom-Up Parsers : The parser in this parsing technique starts 
from the sentence, doing reduction steps, until it reaches the starting
symbol ```S``` of the grammar. The Major problem with this technique
is that the parser doesn't know which substring the parser should select
in each reduction step.

In Top-Down parsing, we have 2 available algorithms for parsing :

1. Recursive Descent Parsing.
2. LL(1) Predictive Parsing.

In Bottom-Up parsing, we have 2 available algorithms for parsing :

1. LR Parsers.
2. Operator Precedence Parsers --> Uses matrix manipulation.

Before we continue, we need to define a few functions

### The FIRST() Function 

Given a string &alpha; &isin; V\*, then 

> FIRST(&alpha;) = { a | &alpha; --\*--> aw, a&isin;V<sub>T</sub>,w&isin;V\*}

in addition, if &alpha; --> &lambda;, then we add &lambda; to FIRST(&alpha;), that is

> &lambda; &isin; FIRST(&alpha;)

That is to say,  FIRST(&alpha;) = Set of all terminals that may begin 
strings derived from &alpha;.

For example 

> &alpha; --\*--> cBx
>
> &alpha; --\*--> ayD
>
> &alpha; --\*--> ab
>
> &alpha; -----> ddd

Then

> FIRST(&alpha;) = {c,a,d}

Assume as well that

> &alpha; --\*--> &lambda;

then 

> FIRST(&alpha;) = {c,a,d,&lambda;}

That is to say, **&lambda; appears in the FIRST() function**.

Lets take an example of this.

### The FOLLOW() Function

We define the FOLLOW() function for **only** nonterminals. That is 
to say 

> FOLLOW(A), A&isin;V<sub>N</sub>

Given 

> S --\*--> uA&beta; , u&isin;V<sub>T</sub>\*, A&isin;V<sub>N</sub>, &beta;&isin;V\*

then

> FOLLOW(A)=FIRST(&beta;)

That is to say, FOLLOW(A) =  The set of all terminals that may appear after A in 
the derivation.

> S --\*--> aaXdd
>
> S --\*--> Xa
>
> S --\*--> BXc

Then 

> FOLLOW(X) = {d,a,c}

### Rules To Compute FIRST() and FOLLOW() Sets

1. FIRST(&lambda;) = {&lambda;}.
2. FIRST(a) = { a }.
3. FIRST(a&alpha;)= {&alpha;}.
4. FIRST(XY) = FIRST(FIRST(X).FIRST(Y)) **OR** FIRST(X.FIRST(Y)) **OR** FIRST(FIRST(X).Y).
5. Given the production A --> &alpha;X&beta;, Then :
	a. FIRST(&beta;) &sub; FOLLOW(X) if &beta; &ne; &lambda;.
	b. FOLLOW(A) &sub; FOLLOW(X) if &beta; = &lambda;.

Note that the FIRST() and FOLLOW() sets are made of **terminals only**

By these rules, say we have 

> A -- > &alpha;X&Beta; , X&isin;V<sub>N</sub>

and say we want FOLLOW(X)

Then 

> FIRST(&Beta;) &sub; FOLLOW(X)

We say it is a subset because we can have other productions involving 
X.

Assuming that &Beta; = &lambda;, Things are different. 

Say that we have a production that leads to this derivation is 

> S --*--> uA&gamma;

and following through this results in this derivation : 

> S --*--> uA&gamma; --> u&alpha;X&gamma;

Therefore, 

> FOLLOW(A) &sub; FOLLOW(X)

This is because whatever follows A can follow X if there is nothing 
between them.

Notes : 

1. &lambda; **may** appear in FIRST() but it doesn't appear in FOLLOW(). We 
will see this when we define augmented grammars.

2. Generally, we start computing the FIRST() from bottom to top, But follow
from top to bottom.

3. When we compute FOLLOW(X), we search for X in the right side of 
any production.

## Augmented Grammars

Given the grammar G=(V<sub>N</sub>,V<sub>T</sub>,S,P), then the 
augmented grammar G\`=(V<sub>N</sub>\`,V<sub>T</sub>\`,S\`,P\`) can 
be obtained from G as follows:

1. V<sub>N</sub>\` = V<sub>N</sub> &cup; {S\`}.
2. V<sub>T</sub>\` = V<sub>T</sub> &cup; { $ }.
3. S\` = new starting point.
4. P` = P &cup; {S\`-->S$}

For example :

> E --> E + T | T
>
> T --> T * F | F
>
> F --> (E) | a

Becomes :

> G --> E$
>
> E --> E + T | T
>
> T --> T * F | F
>
> F --> (E) | a 

This is because we want to create a FOLLOW() set for S.

Lets take another example :

> S\` --> S$
>
> S --> AB
>
> A --> a | &lambda;
>
> B --> b | &lambda;

Lets compute the FIRST() sets for this grammar :

> FIRST(A) = \{a,&lambda;\}
>
> FIRST(B) = \{b,&lambda;\}
> 
> FIRST(S) = FIRST(AB) = FIRST(FIRST(A).FIRST(B))
>
> = FIRST(\{a,&lambda;\},\{b,&lambda;\})
>
> = FIRST(\{a,&lambda;,b,&lambda;})
>
> = \{a,b,&lambda;\}
>
> FIRST(S\`) = FIRST(S$) = FIRST(FIRST(S).FIRST($)) 
>
> = FIRST(\{a,b,&lambda;\}.$)= FIRST(a$,b$,$)
>
> = {a,b,$}

Now Lets compute the FOLLOW() sets for this grammar :

> FOLLOW(S) = \{$\}
>
> FOLLOW(A) = \{b,$\}
>
> FOLLOW(B) = \{$\}

---

Lets Take another, slightly more complex example :

> S\` --> S$
>
> S --> aAcb
>
> S --> Abc
>
> A --> b | c | &lambda;

Lets take the FIRST() for this grammar :

> FIRST(A) = \{b,c,&lambda;\}
>
> FIRST(S) =  FIRST(aAcb)&cup;FIRST(Abc) = \{a,\} &cup; \{b,c\}
>
> = \{a,b,c\}
>
> FIRST(S\`) = FIRST(S$) = FIRST(FIRST(S).FIRST($)) 
>
>= FIRST(\{a,b,c\}.\{$\})
>
> = \{a,b,c\}

Now lets take the FOLLOW() : 

> FOLLOW(S) = \{$\}
>
> FOLLOW(A = \{c,b\}

---

Say we have the grammar 

> G --> E$
>
> E --> E + T | T
>
> T --> T * F | F
>
> F --> (E) | a 

Lets calculate FIRST() :

> FIRST(F) = {(,a}
>
> FIRST(T) = FIRST(T * F)&cup;FIRST(F) =  FIRST(T * F)&cup;{\(,a} 
>
> = {\(,a} (Because every T will eventually become an F)
>
> FIRST(E) = FIRST(E + T) &cup; FIRST(T) = {\(,a} &cup; {\(,a} 
>
> = {\(,a} 
>
> FIRST(G) = FIRST(E$) = {\(,a} 

Now lets Calculate FOLLOW() :

> FOLLOW(E) = \{$,+,)\}
>
> FOLLOW(T) = FOLLOW(E) &cup; \{*\} = \{$,+,\*,)\}
>
> FOLLOW(F) = FOLLOW(T) = \{$,+,\*,)\}

---

But what makes all this so important? 

Well, All of the parsing techniques we are going to learn will heavily rely 
on FIRST() and FOLLOW().

---

### Top-Down Parsing(continued)

#### Recursive Descent Parsing

Recursive Descent Parsing is very simple. It works like this :

1. Divide the grammar into primitive/simple components
	1. For the token "a" :
	  
	    
	   >If(token == "a"){
	   >
	   >		get-next()
	   >
	   >}
	   >
	   >else{
	   >
	   >	report-error()
	   >
	   >}

	2.  For X = &alpha;<sub>1</sub>,&alpha;<sub>2</sub>,...,&alpha;<sub>n</sub> :
		
	 
		>code(X):\{ 
		>
		>code(&alpha;<sub>1</sub>);
		>
		>code{&alpha;<sub>2</sub>};
		>...
		>
		>...
		>
		>...
		>
		>code(&alpha;<sub>n</sub>);
		>\}
		
		Then
		
		> If(token &isin; FIRST(&alpha;<sub>1</sub>))
		>
		>	code(&alpha;<sub>1</sub>)
		>
		> else If(token &isin; FIRST(&alpha;<sub>2</sub>))
		>
		>	code(&alpha;<sub>2</sub>)
		>...
		>
		>...
		>
		>...
		>If(token &isin; FIRST(&alpha;<sub>n</sub>))
		>
		>	code(&alpha;<sub>n</sub>)
		>
		>else{
		>
		> report-error();
		>}
		
		If &alpha; &ne; &lambda;
		
		So we say, the code for x = &alpha;\*, we say
		
		> while(token &isin; FIRST(&alpha;\*))\{
		>
		> call(&alpha\*);
		>
		> /}
---

Notes :

1. Every nonterminal has a code(a function).
2. S\` in augmented grammar is represented by the function "main".
3. We only start with calling "get-token" in function "main".

---

For Example, lets say we have

> G --> E$
>
> E --> T( + T )*
>
> T --> F( \* F )\*
>
> F --> ( E ) | a

Then we can say :

```

 main(){//represents G

 get-token;

 call E();

 if(token!="$")
 {
	Error;
 }
 else{
	SUCCESS;
 }
	
	

}

 function E(){//Represents E -- T (+ T)*

	call T();

	while(token == "+"){

		get-token();

		call T()
	}

 }
 
 function T(){//T--> F (* F)*
 
	call F();
	while(token == "*"){
	
		get-token();
	
		call F();
	}
 }
 
 function F(){
 
	if(token == "(")
	{
		get-token();
		
		call E();
		
		if(token == ")")
		{
			get-token();
		}
		else
		{
			ERROR;
		}
	}
	else if(token=="a")
		{
		get-token();
		}
	else
	{
		ERROR;
	}
 }
 
```

Note that ```ERROR``` is a function we should write.

---

Lets take another example now. 

Given the grammar :

> Program --> body .
> 
> body --> Begin stmt (; stmt)* End
>
> stmt --> Read | Write | body | &lambda;

where we will represent &lambda; as ```l``` from now on in the example;

and

> V<sub>N</sub> = { Program, body, stmt, block}
>
> V<sub>T</sub> = { ., Begin, ;, End, Read, Write}

examples of programs of this language would be

``` 

Begin
	Read;
	Write;
	Read;
	Write;
End.

```
or

```

Begin
	Read;
End.

```

or

```

Begin
	Read;
	Begin
		Read;
		Write;
	End.
	Write;
End.

```

or

```

Begin;
	;
	;
	;
	;
End.

```

Lets write the code for this programming language.

```
main(){

	get-token();
	
	call body();
	
	if(token != "."){
		ERROR;
	}
	else{
		SUCCESS;
	}
}

function body(){

	call Begin();
	if(token == "Begin"){
		get-token();
		call stmt();
		while(token ==";"){
			get-token();
			call stmt();
		}
		if(token == "End"){
			get-token();
		}
		else{
			ERROR;
		}
	
	}
	else{
		ERROR;
	}
}

function stmt(){

	if(token == "Read"){
		get-token();	
	}
	else if(token == "Write"){
		get-token();
	}
	else if(token == "Begin"){
		call body();
	}
	else if(token != ";" || token != "End" ){
		ERROR();
	}
}
		
```

#### LL(1) Parsing

This Parsing method is a **table-driven** parsing method. The LL(1) parsing
table selects which production to choose for the next derivation step. 

But how do we build the LL(1) parsing table?

##### LL(1) Parsing Table Building Algorithm

1. For each production A --> &alpha; in the grammar G, 
   1. Add to the table entry T[A,a] the production A --> &alpha;, where A &isin; FIRST(&alpha;)
2. If &lambda; &isin; FIRST(&alpha;), Add to the table entry T[A,b] the production A --> &alpha; &forall; b &isin; FOLLOW(A).
3. All Remaining Entries are Error Entries.

The table looks like



V<sub>N</sub>\V<sub>T</sub>|test
---|---
A|..

---

For example, given the grammar :

>V --> SR $<sup>1</sup>
>
>S --> +<sup>2</sup> | -<sup>3</sup> | &lambda;<sup>4</sup>
>
>R --> dN.N<sup>5</sup> | .dN<sup>6</sup>
>
>N --> dN<sup>7</sup>| &lambda;<sup>8</sup>

note that the superscript denotes the production number.

Then the table will look like this 

> FIRST(SR $) = {+,-,d,.}
> 
> FIRST(+) = {+}
>
> FOLLOW(S) = {d,.}
>
> FIRST\(R\) = {., $}
>
> FIRST(d) = { - }
>
> FOLLOW(N) = {., $}

V<sub>N</sub>\V<sub>T</sub>| + | - | d | . | $
---|---|---|---|---|---
 V | 1 | 1 | 1 | 1 |
 S | 2 | 3 | 4 | 4 |   
 R |   |   | 5 | 5 |
 N |   |   | 7 | 8 | 8

There should be no conflict(multiple entries) in the LL(1) table.

L(G) of this grammar = all floating point numbers.


The parser works like this

Stack | Remaining Input | Action 
---|---|---|
V    |-dd.d$ | Production 1
SR$  |-dd.d$ | Production 3
-R$  |-dd.d$ | Pop & advance input
R$   | dd.d$ | Production 5
dN.N$| dd.d$ | Pop & advance input
N.N$ |  d.d$ | Production 7
dN.N$|  d.d$ | Pop & advance input
N.N$ |   .d$ | Production 8
.N$  |   .d$ | Pop & advance input
N$   |    d$ | Production 7
dN$  |    d$ | Pop & advance
N$   |     $ | Production 8
$    |     $ | Pop and Advance
&lambda; | &lambda; | Accept 

If at any point the parser reaches a place where the input and the 
stack have 2 different terminal symbols, it throws a syntax error.

---

Lets Take another example. Let the Grammar be :

> program --> block $ <sup>1</sup>
>
> block --> { declarations stmnts } <sup>2</sup>
>
> decls --> D ; decls <sup>3</sup> | &lambda; <sup>4</sup>
>
> stmnts --> statement ; stmts <sup>5</sup> | &lambda; <sup>6</sup>
>
> statement --> if <sup>7</sup> | while <sup>8</sup> | ass <sup>9</sup> | scan <sup>10</sup> | print<sup>11</sup> | block <sup>12</sup> | &lambda;<sup>13</sup>

V<sub>T</sub> = {$,{,},D,;,if,while,ass,scan,print}


V<sub>N</sub>\V<sub>T</sub>|if|while|ass|scan|print|\{|\}|D|;|$
---|---|---|---|---|---|---|---|---|---|---
Program  |   |   |   |   |   | 1 |   |   |   |   
block    |   |   |   |   |   | 2 |   |   |   |   
decls    | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 3 | 4 |   
stmts    | 5 | 5 | 5 | 5 | 5 | 5 | 6 |   | 5 |   
statement| 7 | 8 | 9 | 10| 11| 2 |   |   | 13|   

No conflict. 

